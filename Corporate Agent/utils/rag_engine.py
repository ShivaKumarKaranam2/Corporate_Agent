import os
import requests
import json
from llama_index.core import StorageContext, load_index_from_storage, Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.vector_stores.faiss import FaissVectorStore
from dotenv import load_dotenv

load_dotenv()

# Disable llama_index internal LLM (avoid OpenAI fallback error)
Settings.llm = None

# Path to persisted FAISS store
PERSIST_DIR = r"C:/Users/karan/OneDrive/Documents/Corporate_Agent/ai-engineer-task-ShivaKumarKaranam2/Corporate Agent/utils/vector_store"
EMBED_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"

# Gemini API key and model info
API_KEY = os.getenv("GEMINI_API_KEY")


# Configure embeddings
embed_model = HuggingFaceEmbedding(model_name=EMBED_MODEL_NAME)
Settings.embed_model = embed_model


def load_rag_index():
    """Load FAISS index from disk."""
    if not os.path.exists(PERSIST_DIR):
        raise FileNotFoundError(f"No persisted index found at {PERSIST_DIR}")

    print(f"[RAG] Loading FAISS index from {PERSIST_DIR} ...")
    vector_store = FaissVectorStore.from_persist_dir(PERSIST_DIR)
    storage_context = StorageContext.from_defaults(vector_store=vector_store, persist_dir=PERSIST_DIR)
    return load_index_from_storage(storage_context)


def call_gemini_api(prompt: str) -> str:
    url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent"
    headers = {
        'Content-Type': 'application/json',
        'X-goog-api-key': API_KEY  # Use the API_KEY variable
    }
    payload = {
        "contents": [
            {
                "parts": [
                    {
                        "text": prompt  # Use the prompt parameter
                    }
                ]
            }
        ]
    }

    response = requests.post(url, headers=headers, data=json.dumps(payload)) # Use json.dumps to send payload as JSON
    if response.status_code == 200:
        resp_json = response.json()
        candidates = resp_json.get("candidates", [])
        if candidates:
            # Assuming the output text is in the 'text' field of the first part
            parts = candidates[0].get("content", {}).get("parts", [])
            if parts:
                return parts[0].get("text", "No text output in the first part.")
            else:
                return "No parts found in the first candidate."
        else:
            return "No candidates generated by Gemini."
    else:
        try:
            error_details = response.json()
            return f"Error {response.status_code}: {error_details.get('error', {}).get('message', 'Unknown error')}"
        except json.JSONDecodeError:
            return f"Error {response.status_code}: Could not decode error response."


def query_rag(question: str) -> str:
    """Retrieve context from FAISS and generate answer from Gemini."""
    index = load_rag_index()

    # Retrieve relevant docs (no LLM generation)
    retriever = index.as_retriever()
    retrieved_docs = retriever.retrieve(question)

    # Combine retrieved docs text
    context = "\n\n".join([doc.get_text() for doc in retrieved_docs])

    prompt = (
        f"Use the following context to answer the question.\n\nContext:\n{context}\n\n"
        f"Question:\n{question}\nAnswer:"
    )

    # Call Gemini LLM to generate answer
    answer = call_gemini_api(prompt)
    return answer
